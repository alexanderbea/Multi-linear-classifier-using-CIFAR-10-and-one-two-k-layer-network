{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "st_40Wd1Mux0"
   },
   "source": [
    "# Training a multi-linear classifier with k-layer networks** \n",
    "\n",
    "*Herein, I have trained and tested k-layer networks with multiple outputs to classify images from the CIFAR-10 dataset.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "JkA0wbRlJuFS"
   },
   "outputs": [],
   "source": [
    "#@title Installers\n",
    "# !pip uninstall -y scipy\n",
    "# !pip install scipy==1.2.0\n",
    "# !pip install texttable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "uU3FwIcgRLeB"
   },
   "outputs": [],
   "source": [
    "#@title Import libraries\n",
    "#Import CIFAR-10 data from my google drive folder; I downoaded and unzipped the CIRAR-10 files and uploaded them to my drive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "import pandas\n",
    "import unittest\n",
    "import numpy\n",
    "from collections import OrderedDict\n",
    "import statistics\n",
    "from texttable import Texttable\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from oauth2client.client import GoogleCredentials\n",
    "# Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import numpy as np\n",
    "from googleapiclient.discovery import build\n",
    "drive_service = build('drive', 'v3')\n",
    "\n",
    "import io\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import misc #remove, using PIL instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "BmE517cDWc6x"
   },
   "outputs": [],
   "source": [
    "#@title Functions: Decoding and displaying images\n",
    "def unpickle(file):\n",
    "  dict = pickle.load(file, encoding='bytes')\n",
    "  return dict\n",
    "\n",
    "def unpickle_getFromDrive(file_id):\n",
    "  filename = GetFromDrive(file_id)\n",
    "  dict = pickle.load(filename, encoding='bytes')\n",
    "  return dict \n",
    "\n",
    "def loadLabels(file_id):\n",
    "  data = unpickle_getFromDrive(file_id)\n",
    "  labels = [x.decode('ascii') for x in data[b'label_names']]\n",
    "  return labels\n",
    "\n",
    "def load_batch(file_id):\n",
    "    filename = GetFromDrive(file_id)\n",
    "    dataDict = unpickle(filename)\n",
    "    X = (dataDict[b\"data\"]).T\n",
    "    X = (X - np.mean(X, axis=1, keepdims=True) ) / np.std(X, axis=1, keepdims=True) #transform to have zero mean\n",
    "    y = np.array(dataDict[b\"labels\"])\n",
    "    Y = (np.eye(10)[y]).T   \n",
    "    return X, Y, y\n",
    "\n",
    "def GetFromDrive(file_id): \n",
    "    request = drive_service.files().get_media(fileId=file_id)\n",
    "    downloaded = io.BytesIO()\n",
    "    downloader = MediaIoBaseDownload(downloaded, request)\n",
    "    done = False\n",
    "    while done is False:\n",
    "      _, done = downloader.next_chunk()\n",
    "    downloaded.seek(0)\n",
    "    return downloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sFAklpNpu2s8"
   },
   "source": [
    "- Initialize and store the parameters of my network \n",
    "- Apply the network input vectors and keep a record of the intermediary scores when I apply the network (forward pass)\n",
    "- Compute the gradient of the cost function for a mini-batch relative to the parameters of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "gbTJ1Y_6ueNx"
   },
   "outputs": [],
   "source": [
    "#@title Functions: Load data_batches (1, 2 and test) for training, validation and test\n",
    "def trainOnSmallDataBatch(samples=-1):\n",
    "    X_train, Y_train, y_train = \\\n",
    "        load_batch('1M6-KBxAaqIqy9ekv3vhBlcTd3g1rkFcw') # data_batch_1\n",
    "    X_val, Y_val, y_val = \\\n",
    "        load_batch('155Iy6tGX9HkNgZSdTPc9qGg9HCfay-Fy') # data_batch_2\n",
    "    X_test, Y_test, y_test = \\\n",
    "        load_batch('1HdB9Dv2I9y1K-qcb__9M7Za0ZNtU1aIy') # test_batch\n",
    "\n",
    "    labels = loadLabels('18LLg8Ch3GkdXI0MRAcoSwTzPKdgJMOQv') # labels\n",
    "\n",
    "    if samples > -1:\n",
    "      d = 5 #X_train.shape[0]\n",
    "      data = {\n",
    "          'X_train': X_train[:samples, :d],\n",
    "          'Y_train': Y_train[:samples, :d],\n",
    "          'y_train': y_train,\n",
    "          'X_val': X_val[:samples, :d],\n",
    "          'Y_val': Y_val[:samples, :d],\n",
    "          'y_val': y_val,\n",
    "          'X_test': X_test[:samples, :d],\n",
    "          'Y_test': Y_test[:samples, :d],\n",
    "          'y_test': y_test\n",
    "      }\n",
    "    elif samples == -1:\n",
    "          data = {\n",
    "          'X_train': X_train,\n",
    "          'Y_train': Y_train,\n",
    "          'y_train': y_train,\n",
    "          'X_val': X_val,\n",
    "          'Y_val': Y_val,\n",
    "          'y_val': y_val,\n",
    "          'X_test': X_test,\n",
    "          'Y_test': Y_test,\n",
    "          'y_test': y_test\n",
    "      }\n",
    "\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cwRcNp1QXEh5"
   },
   "source": [
    "The following code is necessary to create my network layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "aDu5AZkpueck"
   },
   "outputs": [],
   "source": [
    "#@title Functions: Create layers\n",
    "def CreateLayers(shapes, activations):\n",
    "    lrs = OrderedDict([])\n",
    "    for i, (shape, activation) in enumerate(zip(shapes, activations)):\n",
    "        lrs[\"layer%s\" % i] = {\"shape\": shape, \"activation\": activation}\n",
    "    return lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "x3vN-URtXvE-"
   },
   "outputs": [],
   "source": [
    "#@title Functions: Initialize network (Change initiazliation herein for testing in the last assignment v)\n",
    "class NetClassifier(): \n",
    "    def __init__(self, data, lbs, layers, alpha=0.8, BN=False):\n",
    "        for k, v in data.items(): setattr(self, k, v)\n",
    "        self.layers     = layers\n",
    "        self.alpha      = alpha\n",
    "        self.lbs     = lbs\n",
    "        self.BN = BN\n",
    "        self.k          = len(layers) - 1\n",
    "        self.activation_funcs = {'relu': self.Relu, 'softmax': self.SoftMax}\n",
    "        self.W, self.b, self.gamma, self.beta, self.mu_V, self.var_V, \\\n",
    "                self.activations = [], [], [], [], [], [], []\n",
    "\n",
    "        for lr in layers.values():\n",
    "            for k, v in lr.items():\n",
    "                if k == \"shape\":\n",
    "                    W, b, gamma, beta, mu_V, var_V = self._init_parameters(v)\n",
    "                    self.W.append(W), self.b.append(b)\n",
    "                    self.gamma.append(gamma), self.beta.append(beta)\n",
    "                    self.mu_V.append(mu_V), self.var_V.append(var_V)\n",
    "                elif k == \"activation\":\n",
    "                    self.activations.append((v, self.activation_funcs[v]))\n",
    "\n",
    "        if self.BN:\n",
    "            self.params = {\"W\": self.W, \"b\": self.b, \"gamma\": self.gamma,\n",
    "                    \"beta\": self.beta}\n",
    "        else:\n",
    "            self.params = {\"W\": self.W, \"b\": self.b}\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_parameters(d):\n",
    "        \n",
    "        stdev = 2/np.sqrt(d[1]) # He initialization whereby weight parameters of each layer are initialized to be normally distributed \n",
    "        # stdev = 1e-1  \n",
    "        # stdev = 1e-3 \n",
    "        # stdev = 1e-4 \n",
    "\n",
    "        W      = np.random.normal(0, stdev, size=(d[0], d[1]))\n",
    "        b      = np.zeros(d[0]).reshape(d[0], 1)\n",
    "        beta   = np.zeros((d[0], 1))\n",
    "        gamma  = np.ones((d[0], 1))\n",
    "        mu_V  = np.zeros((d[0], 1))\n",
    "        var_V = np.zeros((d[0], 1))\n",
    "\n",
    "        return W, b, gamma, beta, mu_V, var_V\n",
    "\n",
    "    @staticmethod\n",
    "    def SoftMax(a):\n",
    "        sm = np.exp(a - np.max(a, axis=0)) / \\\n",
    "                np.exp(a - np.max(a, axis=0)).sum(axis=0)\n",
    "        return sm\n",
    "\n",
    "    @staticmethod\n",
    "    def Relu(a):\n",
    "        a[a<0] = 0\n",
    "        return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B3vrG_TSweXf"
   },
   "source": [
    "The following function (Evalute Classifier, Cost and Accuracy) is from my previous assignment, however, they have been altered in small extent to consider new features of this assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "Dd0U8TjjwgTE"
   },
   "outputs": [],
   "source": [
    "#@title Functions: Evaluate Classifier, Cost and Accuracy\n",
    "class EvaluationFunctions(NetClassifier):\n",
    "    def EvaluateClassifier(self, X, train=False):\n",
    "        s = np.copy(X)\n",
    "\n",
    "        if (self.BN==True):\n",
    "            S, S_h, means, m_var, H = [], [], [], [], []\n",
    "\n",
    "            for i, (W, b, gamma, beta, mu_V, var_V, activation) in enumerate(\n",
    "                    zip(self.W, self.b, self.gamma, self.beta, self.mu_V,\n",
    "                        self.var_V, self.activations)):\n",
    "                H.append(s)\n",
    "                s = W@s + b\n",
    "\n",
    "                if i < self.k:\n",
    "                    S.append(s)\n",
    "                    mu = np.mean(s, axis=1, keepdims=True)\n",
    "                    means.append(mu)\n",
    "                    var = np.var(s, axis=1, keepdims=True) * (X.shape[1]-1)/X.shape[1]\n",
    "                    m_var.append(var)\n",
    "                    if train:\n",
    "                      self.var_V[i] = self.alpha * var_V + (1-self.alpha) * var\n",
    "                      self.mu_V[i]  = self.alpha * mu_V + (1-self.alpha) * mu\n",
    "                    \n",
    "                    s = (s - mu) / np.sqrt(var + np.finfo(np.float64).eps)\n",
    "                    S_h.append(s)\n",
    "                    s = activation[1](beta+np.multiply(gamma, s))\n",
    "\n",
    "                else:\n",
    "                    P = activation[1](s)\n",
    "\n",
    "            return H, P, S, S_h, means, m_var\n",
    "\n",
    "        else:\n",
    "            H = []\n",
    "            for W, b, activation in zip(self.W, self.b, self.activations):\n",
    "                if activation[0] == \"relu\":\n",
    "                    s = activation[1](W@s + b)\n",
    "                    H.append(s)\n",
    "                if activation[0] == \"softmax\":\n",
    "                    P = activation[1](W@s + b)\n",
    "            return H, P\n",
    "\n",
    "    def ComputeAccuracy(self, X, y):\n",
    "        if (self.BN==True):\n",
    "            argM_P = np.argmax(self.EvaluateClassifier(X)[1], axis=0)\n",
    "        else: argM_P = np.argmax(self.EvaluateClassifier(X)[1], axis=0)\n",
    "        acc = argM_P.T[argM_P == np.asarray(y)].shape[0] / X.shape[1]\n",
    "\n",
    "        return acc\n",
    "\n",
    "    def ComputeCost(self, X, Y, lamda): \n",
    "        sW = 0\n",
    "        if self.BN:\n",
    "            _, P, _, _, _, _ = self.EvaluateClassifier(X)\n",
    "        else:\n",
    "            _, P = self.EvaluateClassifier(X)\n",
    "        loss = np.float64(1/X.shape[1]) * - np.sum(Y*np.log(P))\n",
    "        for W in self.W:\n",
    "            sW += (np.sum(np.square(W)))\n",
    "        cost = loss + lamda * sW\n",
    "        return loss, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "98_APNCoyYkS"
   },
   "source": [
    "In the following the network parameters are intialized starting with a 2-layer network. Careful initialization is applied using He initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kTd7UpDxueMJ"
   },
   "outputs": [],
   "source": [
    "data, labels = trainOnSmallDataBatch() \n",
    "layers = CreateLayers(\n",
    "        shapes=[(50, 3072), (10, 50)],\n",
    "        activations=[\"relu\", \"softmax\"])\n",
    "clf = NetClassifier(data, labels, layers) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o8jkWhJ_0l9u"
   },
   "source": [
    "*Compute the gradients for the network parameters*\n",
    "\n",
    "Next, the functions to compute the gradients (copied and altered from my previous assignment) of my k-layer network w.r.t. its weight and bias parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "ENfPd8J2ueJL"
   },
   "outputs": [],
   "source": [
    "#@title Functions: Compute Gradients\n",
    "class ComputeGradients(EvaluationFunctions):\n",
    "  def ComputeGradientsAnalytically(self, X_batch, Y_batch, lamda): \n",
    "    N = X_batch.shape[1]\n",
    "\n",
    "    if self.BN:\n",
    "        grads = {\"W\": [], \"b\": [], \"gamma\": [], \"beta\": []}\n",
    "\n",
    "        for k in self.params:\n",
    "            for par in self.params[k]:\n",
    "                grads[k].append(np.zeros_like(par))\n",
    "\n",
    "        H_b, P_b, S_b, S_h_batch, means_b, vars_b = self.EvaluateClassifier(X_batch, train=True)# Foward\n",
    "        G_b = - (Y_batch - P_b)        # Backward\n",
    "\n",
    "        grads[\"W\"][self.k] = 1/N * G_b@H_b[self.k].T +  2 * lamda * self.W[self.k]\n",
    "        grads[\"b\"][self.k] = np.reshape(1/N * G_b@np.ones(N),(grads[\"b\"][self.k].shape[0], 1))\n",
    "\n",
    "        G_b = self.W[self.k].T@G_b\n",
    "        H_b[self.k][H_b[self.k] <= 0] = 0\n",
    "        G_b = np.multiply(G_b, H_b[self.k] > 0)\n",
    "\n",
    "        for l in range(self.k - 1, -1, -1):\n",
    "            grads[\"gamma\"][l] = np.reshape(1/N * np.multiply(G_b, S_h_batch[l])@np.ones(N), (grads[\"gamma\"][l].shape[0], 1))\n",
    "            grads[\"beta\"][l]  = np.reshape(1/N * G_b@np.ones(N),(grads[\"beta\"][l].shape[0], 1))\n",
    "            G_b = np.multiply(G_b, self.gamma[l])\n",
    "            G_b = self.BN_back_pass(G_b, S_b[l], means_b[l], vars_b[l])\n",
    "            grads[\"W\"][l] = 1/N * G_b@H_b[l].T + 2 * lamda * self.W[l]\n",
    "            grads[\"b\"][l] = np.reshape(1/N * G_b@np.ones(N),(grads[\"b\"][l].shape[0], 1))\n",
    "            if l > 0:\n",
    "                G_b = self.W[l].T@G_b\n",
    "                H_b[l][H_b[l] <= 0] = 0\n",
    "                G_b = np.multiply(G_b, H_b[l] > 0)\n",
    "    else:\n",
    "        grads = {\"W\": [], \"b\": []}\n",
    "        for W, b in zip(self.W, self.b):\n",
    "            grads[\"W\"].append(np.zeros_like(W))\n",
    "            grads[\"b\"].append(np.zeros_like(b))\n",
    "\n",
    "        H_b, P_b = self.EvaluateClassifier(X_batch)# Forward \n",
    "        G_b = - (Y_batch - P_b)# Backward\n",
    "\n",
    "        for l in range(len(self.layers) - 1, 0, -1):\n",
    "            grads[\"W\"][l] = 1/N * G_b@H_b[l-1].T + 2 * lamda * self.W[l]\n",
    "            grads[\"b\"][l] = np.reshape(1/N * G_b@np.ones(N),(grads[\"b\"][l].shape[0], 1))\n",
    "            G_b = self.W[l].T@G_b\n",
    "            H_b[l-1][H_b[l-1] <= 0] = 0\n",
    "            G_b = np.multiply(G_b, H_b[l-1] > 0)\n",
    "\n",
    "        grads[\"W\"][0] = 1/N * G_b@X_batch.T + lamda * self.W[0]\n",
    "        grads[\"b\"][0] = np.reshape(1/N * G_b@np.ones(N), self.b[0].shape)\n",
    "    return grads\n",
    "\n",
    "  def BN_back_pass(self, G_b, S_b, mean_batch, var_batch):\n",
    "      G_1 = np.multiply(G_b, np.power(var_batch + np.finfo(np.float64).eps, -0.5))\n",
    "      D = S_b - mean_batch\n",
    "      c = np.sum(np.multiply(np.multiply(G_b, np.power(var_batch + np.finfo(np.float64).eps, -1.5)), D), axis=1, keepdims=True)\n",
    "      G_b = G_1 - 1/G_b.shape[1] * np.sum(G_1, axis=1, keepdims=True) - 1/G_b.shape[1] * np.multiply(D, c)\n",
    "      return G_b\n",
    "\n",
    "  def ComputeGradientsNum(self, X_batch, Y_batch, size=2, lamda=np.float64(0), h=np.float64(1e-7)):\n",
    "      if (self.BN==True): grads = {\"W\": [], \"b\": [], \"gamma\": [], \"beta\": []}\n",
    "      else:grads = {\"W\": [], \"b\": []}\n",
    "      for j in range(len(self.b)):\n",
    "          for k in self.params:\n",
    "              grads[k].append(np.zeros(self.params[k][j].shape))\n",
    "              for i in range(len(self.params[k][j].flatten())):\n",
    "                  prev_par = self.params[k][j].flat[i]\n",
    "                  self.params[k][j].flat[i] = prev_par + h\n",
    "                  _, f2 = self.ComputeCost(X_batch, Y_batch, lamda)\n",
    "                  self.params[k][j].flat[i] = prev_par - h\n",
    "                  _, f3 = self.ComputeCost(X_batch, Y_batch, lamda)\n",
    "                  self.params[k][j].flat[i] = prev_par\n",
    "                  grads[k][j].flat[i] = (f2-f3) / (2*h)\n",
    "      return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "HXTPOFiesqi-"
   },
   "outputs": [],
   "source": [
    "#@title Functions: Compare Gradients\n",
    "from decimal import *\n",
    "getcontext().prec = 6\n",
    "\n",
    "def CompareGradients():\n",
    "    err = Texttable()\n",
    "    err_data = [] \n",
    "\n",
    "    err_data.append(['Gradient', 'Method',  'Rel Diff Max [e-06]'])\n",
    "\n",
    "    data, labels = trainOnSmallDataBatch()\n",
    "\n",
    "    layers = CreateLayers(\n",
    "            shapes=[(50, 30), (50, 50), (50, 50), (10, 50)],\n",
    "            activations=[\"relu\", \"relu\", \"relu\", \"softmax\"])\n",
    "\n",
    "    clf = ComputeGradients(data, labels, layers)\n",
    "\n",
    "    grads_num = clf.ComputeGradientsNum(\n",
    "            clf.X_train[:30, :5],\n",
    "            clf.Y_train[:30, :5],\n",
    "            lamda=0)\n",
    "    \n",
    "    grads_ana = clf.ComputeGradientsAnalytically(\n",
    "            clf.X_train[:30, :5],\n",
    "            clf.Y_train[:30, :5],\n",
    "            lamda=0)\n",
    "        \n",
    "    num_layers = len(grads_ana[\"W\"])\n",
    "    for l in range(num_layers):\n",
    "        for k in grads_ana:\n",
    "            max_rel_err = max(abs(grads_ana[k][l].flat[:] - grads_num[k][l].flat[:]) / np.asarray([max(abs(a), abs(b)) + 1e-10 for a,b in zip(grads_ana[k][l].flat[:], grads_num[k][l].flat[:])]))\n",
    "            err_data.append([\"layer %d %s\"%(l+1, k), \"ANL vs NUM\", max_rel_err*100*10*100])\n",
    "    \n",
    "    err.add_rows(err_data)\n",
    "    print(\"Method Comparison: Max Err between Analytical vs Numerical\")\n",
    "    print(err.draw())      \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w5F9mQjapy1C"
   },
   "source": [
    "**i) HOW I CHECKED MY ANALYTICAL GRADIENTS**\n",
    "\n",
    "The following generates the gradient comparing result that shows that the implemented analytical gradient method is close enough to be regarded as accurate. The gradients are compared to a numerical method (centered difference method). A four layer neural network without batch normalization is applied on a reduced dataset of 5 images with 30 dimensions. What is noteworthy is that the discrepency between the analytical and numerical gradients increase for the earlier layers as the gradient is back-propogated through the network. Checks are done with no regularization i.e. lambda = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "AlQo85wppxvX"
   },
   "outputs": [],
   "source": [
    "#@title Code: Compare Analytical with Numerical Gradient\n",
    "CompareGradients()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CV7ne39Uc1I0"
   },
   "source": [
    "Moving on, we want to replicate the same test accuracy in a 3 layer network as in the assignment specification of approx. 53.5% after training with the following hyper parameters He initialization and hyper-parameter settings of eta min = 1e-5, eta max = 1e-1, lambda=.005, two cycles of training and n s = 5 * 45,000 / n batch. After training the model 6 times, we achieve approx. 53"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1fy1x092tBxz"
   },
   "source": [
    "Train the network using mini-batch gradient descent and cyclical learning rates and without batch normalization. Firstly, adapt the code from my previous assignment accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "G_j3R0S_tAXi"
   },
   "outputs": [],
   "source": [
    "#@title Functions: Compute Mini Batch Gradient Descent\n",
    "class MBGradientDescent(ComputeGradients):    \n",
    "  def MiniBatchGD(self, X, Y, lamda=0, batch_s=100, eta_min=1e-5,\n",
    "        eta_max=1e-1, n_s = 800, n_epochs=40, plot_id=\"\", plot=False):\n",
    "    if plot:\n",
    "        costs_tr, loss_tr, acc_train, costs_val, loss_val, acc_val = \\\n",
    "                np.zeros(n_epochs), np.zeros(n_epochs), np.zeros(n_epochs), \\\n",
    "                np.zeros(n_epochs), np.zeros(n_epochs), np.zeros(n_epochs)\n",
    "\n",
    "    n_batch = int(np.floor(X.shape[1] / batch_s))\n",
    "    eta = eta_min\n",
    "    t = 0\n",
    "    for n in range(n_epochs):\n",
    "        for j in range(n_batch):\n",
    "            N = int(X.shape[1] / n_batch)\n",
    "            j_start = (j) * N\n",
    "            j_end = (j+1) * N\n",
    "\n",
    "            X_batch = X[:, j_start:j_end]\n",
    "            Y_batch = Y[:, j_start:j_end]\n",
    "\n",
    "            grads = self.ComputeGradientsAnalytically(X_batch, Y_batch, lamda)\n",
    "\n",
    "            for k in self.params:\n",
    "                for par, grad in zip(self.params[k], grads[k]):\n",
    "                    par -= eta * grad\n",
    "\n",
    "            if t <= n_s:eta = eta_min + t/n_s * (eta_max - eta_min)\n",
    "            elif t <= 2*n_s: eta = eta_max - (t - n_s)/n_s * (eta_max - eta_min)\n",
    "            t = (t+1) % (2*n_s)\n",
    "            # print(\"Epoch: \"+str(n) + \" n_batch: \" + str(j) + \" t: \" + str(t))\n",
    "\n",
    "        if plot:\n",
    "            loss_tr[n], costs_tr[n] = self.ComputeCost(X, Y, lamda)\n",
    "            loss_val[n], costs_val[n] = self.ComputeCost(self.X_val, self.Y_val, lamda)\n",
    "            acc_train[n] = self.ComputeAccuracy(self.X_train, self.y_train)\n",
    "            acc_val[n] = self.ComputeAccuracy(self.X_val, self.y_val)\n",
    "\n",
    "\n",
    "    if plot:\n",
    "      self.PlotFigure(n_epochs, costs_tr,  costs_val, \"cost\",y_label=\"cost\", y_max=4.5)\n",
    "      self.PlotFigure(n_epochs, loss_tr, loss_val, \"loss\",y_label=\"loss\", y_max=3.0)\n",
    "      self.PlotFigure(n_epochs, acc_train, acc_val, \"accuracy\",y_label=\"accuracy\", y_max=1.0)\n",
    "\n",
    "    acc_train = self.ComputeAccuracy(self.X_train, self.y_train)\n",
    "    acc_val   = self.ComputeAccuracy(self.X_val, self.y_val)\n",
    "    acc_test  = self.ComputeAccuracy(self.X_test, self.y_test)\n",
    "    \n",
    "    return acc_train, acc_val, acc_test     \n",
    "\n",
    "  def PlotFigure(self, n_epochs, i_train, i_val, title, y_label, y_max=4):\n",
    "        epochs = np.arange(n_epochs)\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        ax.plot(epochs, i_val, 'r-', label=\"validation\")\n",
    "        ax.plot(epochs, i_train, 'g-', label=\"training\")\n",
    "        ax.legend()\n",
    "        ax.set(xlabel='epochs', ylabel=y_label)\n",
    "        ax.set_ylim([0, y_max])\n",
    "        ax.grid()\n",
    "\n",
    "  def PlotTable(self, acc_train, acc_val,acc_test):\n",
    "        t = Texttable()\n",
    "        dd = [] \n",
    "        dd.append(['Train Accuracy', 'Val Accuracy', 'Test Accuracy'])\n",
    "        dd.append([acc_train, acc_val,acc_test])\n",
    "        t.add_rows(dd)\n",
    "        print(\"**********************************************\")\n",
    "        print(t.draw())\n",
    "        print(\" \")               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "r1kw-5AjwYJe"
   },
   "outputs": [],
   "source": [
    "#@title Functions: Load All Data\n",
    "def trainOnAllDataBatches(val):\n",
    "    X_train1, Y_train1, y_train1 = \\\n",
    "        load_batch('1M6-KBxAaqIqy9ekv3vhBlcTd3g1rkFcw')\n",
    "    X_train2, Y_train2, y_train2 = \\\n",
    "        load_batch('155Iy6tGX9HkNgZSdTPc9qGg9HCfay-Fy')\n",
    "    X_train3, Y_train3, y_train3 = \\\n",
    "        load_batch('10VIE8MElRqjIz0z-fIUX80OcWarKDdPp')\n",
    "    X_train4, Y_train4, y_train4 = \\\n",
    "        load_batch('1ht7wULP6aOycu2J5F2zheesizftY2V2b')\n",
    "    X_train5, Y_train5, y_train5 = \\\n",
    "        load_batch('1laJAlpuTD-YR_k9_rE0ZsSrbVApLaWgz')\n",
    "\n",
    "    X_train = np.concatenate((X_train1, X_train2, X_train3, X_train4, X_train5),\n",
    "            axis=1)\n",
    "    Y_train = np.concatenate((Y_train1, Y_train2, Y_train3, Y_train4, Y_train5),\n",
    "            axis=1)\n",
    "    y_train = np.concatenate((y_train1, y_train2, y_train3, y_train4, y_train5))\n",
    "    X_val = X_train[:, -val:]\n",
    "    Y_val = Y_train[:, -val:]\n",
    "    y_val = y_train[-val:]\n",
    "    X_train = X_train[:, :-val]\n",
    "    Y_train = Y_train[:, :-val]\n",
    "    y_train = y_train[:-val]\n",
    "\n",
    "    X_test, Y_test, y_test = \\\n",
    "        load_batch(\"1HdB9Dv2I9y1K-qcb__9M7Za0ZNtU1aIy\")\n",
    "\n",
    "    labels = loadLabels('18LLg8Ch3GkdXI0MRAcoSwTzPKdgJMOQv')\n",
    "    \n",
    "    data = {\n",
    "        'X_train': X_train,\n",
    "        'Y_train': Y_train,\n",
    "        'y_train': y_train,\n",
    "        'X_val': X_val,\n",
    "        'Y_val': Y_val,\n",
    "        'y_val': y_val,\n",
    "        'X_test': X_test,\n",
    "        'Y_test': Y_test,\n",
    "        'y_test': y_test\n",
    "    }\n",
    "\n",
    "    return data, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DnRWW-XJgfKK"
   },
   "source": [
    "Now, the training function is defined where the hyper-parameters setting is n batch=100, eta min = 1e-5, eta max = 1e-1, lambda=.005, two cycles of training and n s = 5 * 45,000 / n batch (2250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "YMJYfW4hvZoU"
   },
   "outputs": [],
   "source": [
    "#@title Functions: Training\n",
    "def Training(layers, BN, lamda=0.005, trainingTimes=1):\n",
    "  data, labels = trainOnAllDataBatches(val=5000)\n",
    "  acc_tr_set = []\n",
    "  acc_val_set = []\n",
    "  acc_tst_set = []\n",
    "  for j in range(trainingTimes):\n",
    "      plotNow = False\n",
    "      if(j==(trainingTimes-1)):\n",
    "        plotNow = True\n",
    "      clf = MBGradientDescent(data, labels, layers, BN=BN)\n",
    "      acc_train, acc_val, acc_test = clf.MiniBatchGD(\n",
    "              data['X_train'], data['Y_train'], lamda=lamda,\n",
    "              batch_s=100, eta_min=1e-5, eta_max=1e-1, n_s=2250,\n",
    "              n_epochs=20,  plot=plotNow)\n",
    "\n",
    "      acc_tr_set.append(acc_train)\n",
    "      acc_val_set.append(acc_val)\n",
    "      acc_tst_set.append(acc_test)\n",
    "\n",
    "  clf.PlotTable(statistics.mean(acc_tr_set), statistics.mean(acc_val_set),statistics.mean(acc_tst_set))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "47Fi4LfkiQ7t"
   },
   "source": [
    "Now run on a 2-layer networ with cyclical learning rate. Here it is clear that the results from assignemnt 2 are replicated. Test accuracy is displayed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xNvw4JHzhm1i"
   },
   "outputs": [],
   "source": [
    "two_layers = CreateLayers(shapes=[(50, 3072), (10, 50)],activations=[\"relu\", \"softmax\"])Training(two_layers, BN=False, trainingTimes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CUfTYKcF32KF"
   },
   "source": [
    "**ii) GRAPHS OF EVOLUTION OF LOSS WITHOUT BATCH NORMALIZATION FOR 3-LAYER NETWORK**\n",
    "\n",
    "Next, a 3-layer network is produced with the same hyper-paramaters and no batch normalization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y5cdHLZ7rmfz"
   },
   "outputs": [],
   "source": [
    "three_layers = CreateLayers(shapes=[(50, 3072), (50, 50), (10, 50)],activations=[\"relu\", \"relu\", \"softmax\"])\n",
    "Training(three_layers, BN=False, trainingTimes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SKsR7Nj_uZV9"
   },
   "source": [
    "**iii) GRAPHS OF EVOLUTION OF LOSS WITHOUT BATCH NORMALIZATION FOR 9-LAYER NETWORK**\n",
    "\n",
    "Next, a 9-layer network is produced with the same hyper-paramaters and no batch normalization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i-BUgzwdsZsd"
   },
   "outputs": [],
   "source": [
    "nine_layers = CreateLayers(shapes=[(50, 3072), (30, 50), (20, 30), (20, 20), (10, 20),(10, 10), (10, 10), \n",
    "                                   (10, 10), (10, 10)],\n",
    "        activations=[\"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"relu\",\"relu\", \"relu\", \"softmax\"])\n",
    "Training(nine_layers, BN=False, trainingTimes=1) # 9-layer model w\\o batch norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UTWhPTYHudXT"
   },
   "source": [
    "EXERCISE 3\n",
    "\n",
    "*Implement batch normalization*\n",
    "\n",
    "**ii) GRAPHS OF EVOLUTION OF LOSS WITH BATCH NORMALIZATION FOR 3-LAYER NETWORK**\n",
    "\n",
    "In the following batch normalization is applied, first to three layer network. If compated to the results above, there is not much difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PH0ZNB5ZwfZs"
   },
   "outputs": [],
   "source": [
    "Training(three_layers, BN=True, trainingTimes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SOKyoPU6wvbx"
   },
   "source": [
    "**iii) GRAPHS OF EVOLUTION OF LOSS WITH BATCH NORMALIZATION FOR 9-LAYER NETWORK**\n",
    "\n",
    "However, as can be observed in the next results, whereby batch normalization is applied to the 9 layer network, it has a signifant impact. It is clear that for deeper neural network, batch normalization is very important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7fyMTn6Qu6C7"
   },
   "outputs": [],
   "source": [
    "Training(nine_layers, BN=True, trainingTimes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "00HxVG4oxqH5"
   },
   "source": [
    "Moving on, we want to replicate the same test accuracy in a 3 layer network as in the assignment specification of approx. 53.5% after training with the following hyper parameters He initialization and hyper-parameter settings of eta min = 1e-5, eta max = 1e-1, lambda=.005, two cycles of training and n s = 5 * 45,000 / n batch. After training the model 6 times, approx. 54% is achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hWkVkkVNyLmM"
   },
   "outputs": [],
   "source": [
    "Training(three_layers, BN=True, trainingTimes=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xGJPdhLsY4lq"
   },
   "source": [
    "**iv) RANGE OF VALUES IN SEARCH FOR LAMBDA FOR 3-LAYER NETWORK WITH BATCH NORMALIZATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zsk0qJ-WysXE"
   },
   "source": [
    "Thereafter, the coarse search for lambda is performed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "BjEm48xAzaR4"
   },
   "outputs": [],
   "source": [
    "#@title Functions: Search for Lambda\n",
    "def searchForLambda(lamda=0.005, trainingTimes=1):\n",
    "  data, labels = trainOnAllDataBatches(val=5000)\n",
    "  layers = CreateLayers(\n",
    "      shapes=[(50, 3072), (50, 50), (10, 50)],\n",
    "      activations=[\"relu\", \"relu\", \"softmax\"])\n",
    "\n",
    "  acc_tst_set = []\n",
    "  acc_tr_set = []\n",
    "  acc_val_set = []\n",
    "  for j in range(trainingTimes):\n",
    "      clf = MBGradientDescent(data, labels, layers, BN=True)\n",
    "      acc_train, acc_val, acc_test = clf.MiniBatchGD(\n",
    "              data['X_train'], data['Y_train'], lamda=lamda,\n",
    "              batch_s=100, eta_min=1e-5, eta_max=1e-1, n_s=2250,\n",
    "              n_epochs=20,  plot=False)\n",
    "\n",
    "      acc_tr_set.append(acc_train)\n",
    "      acc_val_set.append(acc_val)\n",
    "      acc_tst_set.append(acc_test)\n",
    "\n",
    "  tr_mean_acc = str(statistics.mean(acc_tr_set))\n",
    "  val_mean_acc =  str(statistics.mean(acc_val_set))\n",
    "  tst_mean_acc =  str(statistics.mean(acc_tst_set))\n",
    "  \n",
    "  return tr_mean_acc, val_mean_acc, tst_mean_acc\n",
    "\n",
    "def getResultsWhenTrainingForDifferentLambdas(lambdas, title):\n",
    "    t = Texttable()\n",
    "    data = [] \n",
    "    data.append(['Parameters', 'Train Accuracy', 'Val Accuracy', 'Test Accuracy'])\n",
    "\n",
    "    for x in range(0, len(lambdas)):\n",
    "      train_mean_acc, val_mean_acc, test_mean_acc = searchForLambda(lamda=lambdas[x])\n",
    "      saveFortbl = \"lambda=\"+str(lambdas[x])\n",
    "      data.append([saveFortbl,train_mean_acc, val_mean_acc,test_mean_acc])\n",
    "      print(\"Lambda test done:\" + str(lambdas[x]))\n",
    "    t.add_rows(data)\n",
    "    print(\"********************* \"+title +\" *************************\")\n",
    "    print(\"n epochs=20\")\n",
    "    print(\"*************************************************************\")\n",
    "    print(t.draw())\n",
    "    print(\" \")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "bl0ZPEAgzM2x"
   },
   "outputs": [],
   "source": [
    "#@title Code: Coarse Search\n",
    "lambdas = [.0100, .0200, .0300, .0400, .0500, .0600] #random selection of very low regularization\n",
    "getResultsWhenTrainingForDifferentLambdas(lambdas, \"COARSE SEARCH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OV0q7imhSXEM"
   },
   "source": [
    "In the above coarse search I randomly selected five very low regularzation terms. In the result table it becomes clear that the validation accuracy is highest where lambda is below 0.02, thereby the fine search will be between 0.005 and 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pGOBU9wRzHu-"
   },
   "source": [
    "Continuing, the fine search for lambda is performed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "hrz5I6kHzN-P"
   },
   "outputs": [],
   "source": [
    "#@title Code: Fine Search\n",
    "lambdas = [.0051, .0063, .0069, .0094, .0134, .0154] #random selection of very low regularization\n",
    "getResultsWhenTrainingForDifferentLambdas(lambdas, \"FINE SEARCH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zjh17VmdUK2C"
   },
   "source": [
    "The best fine search that yielded the highest accuracy on the validation set was at lambda=0.0134 as is displayed in the table below. For future work, I would explore running a much wider random search. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eplFvRbqzPAa"
   },
   "source": [
    "Finally, the optimal lambda is implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EdqCVFf9wmZC"
   },
   "outputs": [],
   "source": [
    "Training(three_layers, BN=True, trainingTimes=6, lamda=0.0134)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d-hCavsAW4Sb"
   },
   "source": [
    "**v) SENSITIVITY TO INITIALIZATION**\n",
    "\n",
    "Next we need to implement a sensitivity analysis to the initialization where I experiment on training with batch normalization and without it. Instead of He initialization I will apply sig=1e-1, 1e-3 and 1e-4. In order to test this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CP61LeWPtu8V"
   },
   "source": [
    "He initialization without BN (change on the top in order to test sig=1e-1, 1e-3 and 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U-jZ4-GHt62W"
   },
   "outputs": [],
   "source": [
    "Training(three_layers, BN=False, trainingTimes=6, lamda=0.0134)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9jM6Y6zhtxDv"
   },
   "source": [
    "He initialization with BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ue0_scMrt5VI"
   },
   "outputs": [],
   "source": [
    "Training(three_layers, BN=True, trainingTimes=6, lamda=0.0134)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JbfRp8OYvId0"
   },
   "source": [
    "As can be observed in the loss plots above, the networks are less sensitive to weight initialization when I train with batch normalization. It be concluded that batch normalization helps significantly the network to learn even if we start of with a bad initialization. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Training a multi-linear classifier with k-layer networks.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
